\documentclass[DM,authoryear,toc,lsstdraft]{lsstdoc}
\usepackage[nonumberlist,nogroupskip,toc,numberedsection=autolabel]{glossaries}
\makeglossaries
\input{glossary}

\input meta.tex

\title{DM QA Status \& Plans}

\author{%
Simon Krughoff,
John Swinbank
}

\setDocRef{\lsstDocType-\lsstDocNum}
\date{\vcsDate}

\setDocAbstract{%
This document will:

\begin{itemize}

  \item{Describe the current status of ``\gls{qa}'' tools, in the broadest sense,
  currently provided by Data Management;}

  \item{Lay out a set of common use cases and requirements for future QA
  tool and service development across the subsystem.}

\end{itemize}
}

% Change history defined here.
% Order: oldest first.
% Fields: VERSION, DATE, DESCRIPTION, OWNER NAME.
% See LPM-51 for version number policy.
\setDocChangeRecord{%
  \addtohist{1}{YYY-MM-DD}{Unreleased.}{Simon Krughoff}
}

\begin{document}

\maketitle

\section{Introduction}
\label{sec:intro}

Across the Data Management subsystem, we (ab)use the term ``QA'' to refer to
various aspects of ensuring that things are ``working properly''. This spans a
wide gamut of applications, including, for example:

\begin{itemize}

\item{Does our code correctly compile and pass its unit tests?}
\item{Can we demonstrate that the DM system meets \glspl{kpm} and satisfies
other aspects of our requirements documentation (\citeds{LSE-29, LSE-30,
LSE-61})?}
\item{Do we properly understand the operation of our scientific algorithms,
both individually and operating in concert? Can we identify when the results
they produce are scientifically lacking?}
\item{Do we provide tools to developers, scientists and other members of the
DM team to help them understand and debug the code and systems they are
constructing or using as part of their work?}
\item{How do we track computational performance across the system, from
execution times of scientific algorithms to the scaling properties of database
queries?}
\item{Can we monitor systems within the LSST Data Facility to ensure that they
are operational and performing correctly?}
\item{Can we identify problems which stem from bad data, as distinct from bad
software or services?}

\end{itemize}

To date, the DM team has build a number of tools which address parts of these
problems. However, a coherent, unified vision for how they fit together
remains lacking. This document will\footnote{Ultimately; the current draft
does yet address all aspects of this scope!} catalogue the tools that are currently
available, will identify use-cases and the requirements arising from them,
will determine to what extent the existing tools satisfy those requirements,
and will suggest directions for future development.

\section{Current Tooling}
\label{sec:current}

We begin by cataloguing the tooling which has been developed to date, or which
will be deployed in the short term\footnote{That is, which we can count on
becoming available shortly, regardless of any future course corrections which
result from this document or other discussion.}. These tools are listed by the
team within DM which originated or leads the development of them.

\subsection{Alert Production (AP)}
\label{sec:current:ap}

\subsubsection{ap\_pipe and ap\_verify}

Development within the Alert Production group has focused on the construction
of an instrumented ``end-to-end'' alert production pipeline.

The pipeline code itself lives in the \code{lsst-dm/ap\_pipe} package in
GitHub. This provides prototype implementations of major alert production
pipeline components (single frame processing, image differencing, source
association), and a control script to string them together\footnote{At time of
writing, this system is being migrated to use the DM-standard
\code{CmdLineTask} framework.}

The \code{lsst-dm/ap\_verify} package is a companion to ap\_pipe. ap\_verify
effectively wraps pipeline functionality in a form that is intended to be
appropriate for regular testing in CI (\S\ref{sec:current:square:ci}). As
such, it provides:

\begin{itemize}

  \item{A standardized way of defining ``dataset'' packages, each of which
  provide a curated test dataset;}
  \item{The facility to ingest data into Butler repositories suitable for
  processing with LSST stack tools;}
  \item{Facilities for instrumenting and calculating \glspl{metric} based upon
  running pipeline code;}
  \item{Submission of calculated metrics to SQuaSH using the lsst.verify
  system (\S\S\ref{sec:current:square:squash} \&
  \ref{sec:current:square:verify}).}

\end{itemize}

Based largely upon the experience gained in building ap\_verify, the AP team
has put considerable thought into the appropriate mechanisms for extracting
``metrics'' from running pipeline \code{Task}s. This has resulted in
\citeds{DMTN-057}, which describes a number of possibilites for how this might
be standardized. At time of writing, none of these proposals have been
formally adopted by the project.

\subsection{Data Release Production (DRP)}
\label{sec:current:drp}

\subsubsection{afwDisplay}
\label{sec:current:drp:afwDisplay}

\begin{draftnote}
Not clear that this is really a DRP deliverable, but I think they are probably
the key users and developers.
\end{draftnote}

\subsubsection{ci\_hsc}
\label{sec:current:drp:cihsc}

The ci\_hsc package provides a curated set of HSC data and a
SCons\footnote{\url{http://www.scons.org/}; note that the choice of SCons here
is ingenious --- it provides many desirable features in a workflow system ---
but also completely non-standard across the rest of the codebase, and can make
this package hard for developers to engage with.}-based system for processing
it through a DRP-like workflow. The resulting data products are automatically
sanity-checked: that is, we establish that the expected outputs have been
produced and contain a plausible number of objects, reasonable selection of
objects used for PSF determination, and so on, but we do not do detailed
analysis of source measurements or astrophyisical plausibility. This means
that ci\_hsc provides an excellent way to catch regressions in pipeline
machinery and integration, but is not sensitive to more subtle algorithmic
issues.

ci\_hsc may be run standalone by individual developers --- it takes around
three hours --- and is periodically run by the CI system
(\S\ref{sec:current:square:ci}).

\subsubsection{Automated static plots with pipe\_analysis}
\label{sec:current:drp:pipeanalysis}

% Text adapted from input from Yusra & the DRP group.
The pipe\_analysis package provides scripts which inspect a repository of
processed data and generate static plots of the distributions of
scientifically relevant quantities. These may be manually inspected to
demonstrate the internal internal consistency and fidelity of photometric and
astrometric measurements on the visit-stage and coadd-stage catalog outputs.
They also can be used to compare catalog outputs from two different reruns

The plots generated by pipe\_analysis have been refined, and new plots added
to the collection, based on several years of investigating issues encountered
by the DRP group in processing HSC data.

\begin{figure}
\begin{center}
\includegraphics[width=0.48\textwidth]{figures/plot-t9813-color_wPerp-psfMagHist.png}
\includegraphics[width=0.48\textwidth]{figures/plot-t9813-rizDistancePSF-sky-stars.png}
\end{center}
\caption{Examples of static plots generated by pipe\_analysis, courtesty of Yusra
AlSayyad.}
\label{fig:pipeanalysis}
\end{figure}

Examples of plots produced by pipe\_analysis are shown in Figure
\ref{fig:pipeanalysis}. Note that this figure shows but two of many plots
generated.

\subsubsection{Dynamic ``drill-down'' plots}
\label{sec:current:drp:drilldown}

% Text adapted from input from Yusra & the DRP group.
Tim Morton (Princeton) is currently building a toolset that offers the ability
to create plots of tracts-worth of catalog data to find patterns and
pathologies. These interactive and linked visualizations, inspired by the
pipe\_analysis plots (\S\ref{sec:current:drp:pipeanalysis}), are created and
explored live in a Jupyter notebook environment.

The current system plotting of multiple quantities of interest, linked to each
other and to sky maps of each quantity; scanning through sky plots of a
quantity from each visit contributing to a coadd; and in-notebook quick-look
of images corresponding to catalog data points.

This toolset is being designed with scalability in mind, and is easily capable
of handling millions of datapoints.

\subsubsection{Large scale scientific analysis of HSC data}
\label{sec:current:drp:hsc}

% Text adapted from input from Yusra & the DRP group.
Approximately annual, a derivate of the LSST codebase is used to process the
full collection of data from the HSC Strategic Survey Program.  This is
coupled with an intensive QA effort (using several of the tools listed above,
in addition to pipeline production scientists in Japan looking at the data) to
verify that new desired features are working as expected and that the
scientific performance of the pipeline has not regressed, before the results
are released to the HSC community. These data releases form the basis of
scientific analyses by the HSC collaboration, which in turn identify issues
that may go unnoticed during pipeline processing.

Processing the entire survey dataset in this way enables the identification of
edge and corner cases that may otherwise go unnoticed.  Unfortunately, the
pipeline outputs are proprietary until they are released to the world a couple
of years later, but members of the DRP team at Princeton can access them and
use the results to trigger development and bug-fixes using public data.

\subsection{Science User Interface \& Tools (SUIT)}
\label{sec:current:suit}

\subsubsection{Firefly}
\label{sec:current:suit:firefly}

\href{https://github.com/Caltech-IPAC/firefly}{Firefly} is the IPAC's ``advanced
astronomy web UI framework''. It provides data browsing, image viewing, and
plotting functionality, and will be the focus of the ``portal aspect'' of the
Science Platform.

Firefly is a client-server application; a Java-based server component is
colocated with the data, and is accessed through a Javascript based UI in the
user's browser. At various times, installations of Firefly have been made
available to DM developers on project-provided compute hardware\footnote{i.e.
\texttt{lsst-dev01.ncsa.illionois.edu}}, but these are not regularly
maintained or accessed by pipelines developers.

Firefly interfaces to the standard image display system used by the LSST
pipelines (\S\ref{sec:current:drp:afwDisplay}) are available.

The functionality available from Firefly may be useful to other DM teams, in
particular providing visualization and plotting services in support of
algorithm development and investigating the properties of data releases.
However, the SUIT team is not scoped to deliver capabilities to the rest of
DM: they are focused on developing Firefly to meet the requirements of the
Science Platform. Other parts of DM may, of course, benefit from the tools
developed for this purpose.

\subsection{Science Data Archive \& Application Services (DAX)}
\label{sec:current:dax}

\subsubsection{Database intgration testing}
\label{sec:current:dax:dbint}

% Text adapted from input from Fritz.
Qserv has a multi-node integration test which spins up several shard servers
and a master via containers, loads a test dataset, and issues a suite of
queries checking against known/expected results. This is run automatically
using the \href{https://travis-ci.org}{Travis CI} service on all changes.

Unfortunately, the nature of this test and the Travis CI service renders it
complicated and brittle; these tests are often broken for reasons unrelated to
a developer's changes. The same test can, however, be run manually by
individual developers before merging.

Larger-scale QServ integration tests are carried out periodically at CC-IN2P3
and the LSST Data Facility. These consist of deploying the containerized Qserv
system across a large cluster and executing queries against large-scale test
datasets. These tests are currently controlled by a collection of scripts, but
work is underway to move to a Kubernetes\footnote{https://kubernetes.io}-based
infrastructure.

\subsubsection{Database performance testing}
\label{sec:current:dax:dbperf}

% Text adapted from input from Fritz.
Performance testing for qserv is performed annual once yearly, on synthetic
data sets that are on a glide path to the scale of LSST DR1. The test
procedure is described in \citeds{LDM-552}; examples of the reports produced
include \citeds{DMTR-13} and \citeds{DMTR-16}.

\subsubsection{DAX services}

\begin{warning}
TBD.
\end{warning}

\subsubsection{\code{lsstDebug}}
\label{sec:current:dax:debugopt}

% Text adapted from input from Yusra & the DRP group.
The author of an algorithmic task knows which intermediate data products and
diagnostic plots are useful for answering questions about its behavior. The
\code{lsstDebug} framework allows developers to insert code into their
\code{Task}s that displays images (using standard DM primitives) and makes
plots that is only run when the \code{CmdLineTask} is invoked with the
\code{--debug} parameter.

Historically, the \code{lsstDebug} system has been poorly documented and the
subject of some confusion. There are no published guidelines about appropriate
ways to use it or expectations of how developers should instrument their
\code{Task}s.

Note that the \code{lsstDebug} system did not originate with the DAX team
(and, indeed, it's quite likely that no member of DAX has ever used it), but
it falls within their remit as part of the ``task framework''.

\subsection{Data Facility (LDF)}
\label{sec:current:ldf}

\subsubsection{Regular manual reprocessing of HSC data}
\label{sec:current:ldf:hsc}

% Text adapted from input from Yusra & the DRP group.
Every two weeks, members\footnote{Principally Hsin-Fang Chiang} of the LDF
team reprocess some three tracts of HSC data, comprising the ``RC2''
dataset\footnote{The RC2 dataset is fully defined in \jira{DM-11345}.},
through the current weekly release of the DRP pipeline\footnote{Currently,
this means \code{makeSkyMap.py}, \code{singleFrameDriver.py},
\code{mosaic.py}, \code{skyCorrection.py}, \code{coaddDriver.py} and
\code{multiBandDriver.py}.}, including post-processing with the pipe\_analysis
system (\S\ref{sec:current:drp:pipeanalysis}). Any substantive changes in
processing outputs\footnote{For example, changes to CCDs logged as failing
certain pipeline steps, or new errors or exceptions logged during pipeline
operation; we do not consider changes to the contents of the science data
products ``substantive'' for this purpose, as long as those data products are
generated as expected.} are flagged for the attention of the DRP team. The
results of the four most recent processing campaigns are retained for
reference.

In addition, Data Facility staff periodically, on request from the DRP team,
reprocess the whole of the first HSC public data release (``PDR1'') on the
Verification Cluster.

\subsection{Science Quality and Reliability Engineering (SQuaRE)}
\label{sec:current:square}

\subsubsection{Continuous Integration services (Jenkins)}
\label{sec:current:square:ci}

SQuaRE provides and maintains the CI system which regularly builds and
tests much of the DM codebase\footnote{Principally, CI covers products of
the Science Pipelines and Database teams; I believe that little-to-none of the
UI code developed by SUIT, of the service management code developed by LDF, or
of the code used to provide services by SQuaRE, is regularly tested in this CI
system.}.

The CI system is widely used for a number of related purposes, including:

\begin{itemize}

  \item{Testing of code by developers before it is merged;}
  \item{Executing metric collection systems (e.g. validate\_drp,
  \S\ref{sec:current:square:validate});}
  \item{Building and preparing code for public release.}

\end{itemize}

\subsubsection{Unit test framework}

All code written for DM is expected to be accompanied by appropriate unit
tests\footnote{Refer to the
\href{https://developer.lsst.io/coding/unit_test_policy.html}{Unit Test
Policy} in the \href{https://developer.lsst.io}{Developer Guide}, but note
that our implementation does not follow the terminology it uses}.

The ideal unit test demonstrates that an individual ``unit'' of software (a
class, a function, etc) is operating correctly. In practice, many tests used
by DM go well beyond this, for obvious reasons: it's hard to demonstrate that
the \code{ProcessCcdTask} class is operating correctly without relying on
classes for loading data, performing instrument signature removal, source
detection and measurement, calibration, and so on. As a natural result of
this, DM's unit tests are often (ab)used to provide mini-integration tests.
This can cause fragility in the test suite, but also provides convenient
test capabilities which are not easily available elsewhere.

Although the creation of unit tests is not a responsibility of SQuaRE, the
technology used to implement them, and their regular execution as part of the
CI system, is (although we note that much of the current test harness has been
developed in close cooperation with the Architecture team).

\subsubsection{Stack Demo}
\label{sec:current:square:demo}

The Stack Demo, \code{lsst/lsst\_dm\_stack\_demo}, contains a curated
selection of SDSS data, a script which executed single frame measurement on
that data, and a set of precomputed expected outputs. The results of
measurement are compared with the expected values, and an error is thrown if
any values (including number of detected sources, their positions, and
assorted measurement algorithm results) are different.

Note that the expected values are the results of some previous processing run.
There is no ground truth for this data, and these results have not been
rigorously analysed to demonstrate that they are correct. In short, this
procedure can verify that changes being made do not substantially change the
results of processing, but cannot verify that the results are intrinsically
correct.

The Stack Demo is available for end users to run standalone, but it is also
automatically run as part of most CI (\S\ref{sec:current:square:ci}) jobs.

\subsubsection{lsst.verify}
\label{sec:current:square:verify}

% text adapted from
% https://pipelines.lsst.io/v/DM-11597/modules/lsst.verify/index.html

\href{https://github.com/lsst/verify}{lsst.verify} is a framework for packages
that measure software and data quality ``metrics''. A metric can be any
measurable scalar quantity; some examples are in the LSST Science Requirements
Document (\citeds{LPM-17}), though packages can also define ad hoc metrics.
Measurements made through lsst.verify can be uploaded to LSST's SQuaSH
monitoring dashboard (\S\ref{sec:current:square:squash}).

\subsubsection{SQuaSH}
\label{sec:current:square:squash}

\href{https://squash.lsst.codes}{SQuaSH} is a metric aggregation and display
service. It provides time-series visualization of selected metrics derived
from \citeds{LDM-502}. Metics may be submitted to SQuaSH using lsst.verify
(\S\ref{sec:current:square:verify}).

\begin{draftnote}
I --- JDS --- am actually unsure of the bigger picture of SQuaSH beyond that.
I know there have been plans for end users to extend it to plot arbitrary
metrics, but I don't know if they've been implemented, and I know work has
been done on more advanced visualizations, but I don't know how much of that
has rolled out or where it's heading next. Simon, perhaps you can expand?
\end{draftnote}

SQuaSH is described in \citeds{SQR-009}.

\subsubsection{Validation packages}
\label{sec:current:square:validate}

\begin{draftnote}
I --- JDS --- am unsure as to whether these packages are actually a product of
SQuaRE or of DM Science or of System Level Testing \& Science Validation
(02C.09). Simon, do you know?
\end{draftnote}

The validate\_drp package will automatically run single-frame processing on a
dataset, calculate a subset of metrics derived from the Science Requirements
Document (\citeds{LPM-17}), upload the results to SQuaSH
(\S\ref{sec:current:square:squash}, and evaluate expected analytic models for
photometric and astrometric performance following \cite{2008arXiv0805.2366I}.

Currently, validate\_drp supports the following metrics\footnote{From
\code{etc/metrics.yaml} of validate\_drp \code{w.2018.07}}:

\begin{itemize}
\item{Relative astrometry
  \begin{itemize}
    \item{AM1, AF1, AD1}
    \item{AM2, AF2, AD2}
    \item{AM3, AF3, AD3}
  \end{itemize}
}
\item{Photometric repeatability
  \begin{itemize}
    \item{PA1, PA2, PF1}
  \end{itemize}
}
\item{Residual PSF ellipticity correlations
  \begin{itemize}
    \item{TE1, TE2}
  \end{itemize}
}
\end{itemize}

In addition, three curated datasets for use with validate\_drp are provided.
These are based on CFHT (validation\_data\_cfht), DECam
(validation\_data\_decam) and HSC (validation\_data\_hsc). These datasets are
reguarly reprocessed by the CI system (\S\ref{sec:current:square:ci}) and the
results uploaded to SQuaSH.

Note that the validation\_data packages contain both ``raw'' data and
processed data which has been ingested to a Butler repository. There is no
regular routine for updating that processed data in light of changes to the
software; developers have occasionally found it unclear or confusing whether
this data is actually supposed to be usable or useful.

\subsubsection{Hosted Jupyter notebooks}
\label{sec:current:square:jupyter}

\begin{draftnote}
I --- JDS --- know they exist at some level, but I'm not sure of any details.
As I understand it, to date their development has entirely(??) been driven by
a desire to support commissioning, rather than for internal use by DM (at
least, I'm not aware of any requirements gathering from other DM teams...
unless that was covered by the Science Platform?). Simon, can you add a note
here describing what's available now and what the short-term plans for it are?
\end{draftnote}

\section{Use Cases and Requests}
\label{sec:use}

\begin{draftnote}
At least for now, I --- JDS --- will dump relatively unstructured requests,
comments and use cases that we've collected so far. Later, we should try to
bring structure to this chaos.

May be worth drawing attention to
\href{https://github.com/lsst-dm/dmtn-074/commit/06fa883ebeb12cf728dda14fa82e4c1f01fa5696#r27754730}{this
comment from KTL}.
\end{draftnote}

In this section, we present a selection of use cases in an attempt to build a
relatively comprehensive overview of the needs for QA tools, services and
procedures within DM. We will consider four broadly separate ``regimes'' in
which QA procedures of one sort or another are relevant:

\begin{description}

\item[Developer support]{Provide developers with the tools they need to work
quicky and effectively when developing scientific algorithms and/or other core
pieces of the system.}

\item[Code quality]{Ensure that our code is buildable, executable, and
maintainable.}

\item[Metric verification]{Demonstrate that we can reliably execute code at
scale and track its performance against predefined targets.}

\item[Science validation]{Check that the results of processing are
scientifically useful, and characterize all the data products produced.}

\end{description}

\subsection{Developer Support}

\begin{itemize}

\item{Availability of test data}
\item{Availability of plotting frameworks}
\item{Instrumented tasks}
\item{...}

\end{itemize}

\subsection{Code quality}

\begin{itemize}
\item{Unit tests; better coverage.}
\item{Integration tests.}
\end{itemize}

\subsection{Metric verification}

\begin{itemize}
\item{...}
\end{itemize}

\subsection{Science validation}

\begin{itemize}
\item{...}
\end{itemize}

\section{Requirements}
\label{sec:req}

\printglossary[style=index]

\bibliography{lsst,lsst-dm,refs_ads,refs,books}

\end{document}
